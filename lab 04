import tensorflow as tf 
import numpy as np 

tf.enable_eager_execution()
tf.__version__ 

tf.set_random_seed(0)  # for reproducibility 

x1_data = [1, 0, 3, 0, 5] #X1 값의 데이터
x2_data = [0, 2, 0, 4, 0] #X2 값의 데이터
y_data  = [1, 2, 3, 4, 5] #y 값의 데이터

W1 = tf.Variable(tf.random_uniform([1], -10.0, 10.0)) # 파라미터 업데이트
W2 = tf.Variable(tf.random_uniform([1], -10.0, 10.0)) # 파라미터 업데이트
b  = tf.Variable(tf.random_uniform([1], -10.0, 10.0)) # 파라미터 업데이트

learning_rate = tf.Variable(0.001) # learning rate 값 설정

for i in range(1000+1): # 1000+1번 반복
    with tf.GradientTape() as tape: # 경사하강법
        hypothesis = W1 * x1_data + W2 * x2_data + b 
        cost = tf.reduce_mean(tf.square(hypothesis - y_data)) # 감소값 평균
    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b])
    W1.assign_sub(learning_rate * W1_grad) # 파라미터 업데이트
    W2.assign_sub(learning_rate * W2_grad) # 파라미터 업데이트
    b.assign_sub(learning_rate * b_grad) #파라미터 업데이트

    if i % 50 == 0:
        print("{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}".format(
          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))

 x_data = [[1., 0., 3., 0., 5.], [0., 2., 0., 4., 0.]] # X 값의 데이터
y_data  = [1, 2, 3, 4, 5] # y 값의 데이터

W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0)) # 파라미터 업데이트
b = tf.Variable(tf.random_uniform([1], -1.0, 1.0)) # 파라미터 업데이트

learning_rate = tf.Variable(0.001) # learning rate 업데이트

for i in range(1000+1):# 1000+1번 반복
    with tf.GradientTape() as tape: # 경사하강법
        hypothesis = tf.matmul(W, x_data) + b # (1, 2) * (2, 5) = (1, 5)
        cost = tf.reduce_mean(tf.square(hypothesis - y_data)) # cost함수
        W_grad, b_grad = tape.gradient(cost, [W, b]) 
        W.assign_sub(learning_rate * W_grad) # 파라미터 업데이트
        b.assign_sub(learning_rate * b_grad) # 파라미터 업데이트

    if i % 50 == 0:.
        print("{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}".format(
            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))

import tensorflow as tf 

# bias(b)를 행렬에 추가함
x_data = [
    [1., 1., 1., 1., 1.], # bias(b)
    [1., 0., 3., 0., 5.], 
    [0., 2., 0., 4., 0.] # X 값의 데이터
]
y_data  = [1, 2, 3, 4, 5] # y 값의 데이터

W = tf.Variable(tf.random_uniform([1, 3], -1.0, 1.0)) 

learning_rate = 0.001 # learning rate 설정
optimizer = tf.train.GradientDescentOptimizer(learning_rate) 

for i in range(1000+1): # 1000+1번 반복
    with tf.GradientTape() as tape: # 경사하강법
        hypothesis = tf.matmul(W, x_data) 
        cost = tf.reduce_mean(tf.square(hypothesis - y_data)) # cost함수

    grads = tape.gradient(cost, [W]) 
    optimizer.apply_gradients(grads_and_vars=zip(grads,[W]))
    if i % 50 == 0:
        print("{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}".format(
            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))

# Multi-variable linear regression (1)

X = tf.constant([[1., 2.], 
                 [3., 4.]]) 
y = tf.constant([[1.5], [3.5]]) 

W = tf.Variable(tf.random_normal([2, 1]))
b = tf.Variable(tf.random_normal([1]))

# Create an optimizer
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)

tf.set_random_seed(0)  # for reproducibility 

data = np.array([
     # X1,   X2,    X3,   y
    [ 73.,  80.,  75., 152. ],   
    [ 93.,  88.,  93., 185. ],
    [ 89.,  91.,  90., 180. ],
    [ 96.,  98., 100., 196. ],
    [ 73.,  66.,  70., 142. ]
], dtype=np.float32)

# slice data
X = data[:, :-1]
y = data[:, [-1]]

W = tf.Variable(tf.random_normal([3, 1]))  
b = tf.Variable(tf.random_normal([1])) 

learning_rate = 0.000001 # learning rate 설정


# hypothesis, prediction function
def predict(X):
    return tf.matmul(X, W) + b #예측

print("epoch | cost")

n_epochs = 2000
for i in range(n_epochs+1):
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:
        cost = tf.reduce_mean((tf.square(predict(X) - y)))

    # calculates the gradients of the loss
    W_grad, b_grad = tape.gradient(cost, [W, b])

    # updates parameters (W and b)
    W.assign_sub(learning_rate * W_grad) # 파라미터 업데이트
    b.assign_sub(learning_rate * b_grad) # 파라미터 업데이트

    if i % 100 == 0:  
        print("{:5} | {:10.4f}".format(i, cost.numpy()))

W.numpy()
b.numpy()
tf.matmul(X, W) + b
Y # 실제값
predict(X).numpy() # 예측값
# 예측
predict([[ 89.,  95.,  92.],[ 84.,  92.,  85.]]).numpy()
